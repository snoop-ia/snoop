{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T14:02:41.666699Z",
     "start_time": "2024-06-04T14:02:41.073785Z"
    }
   },
   "source": [
    "from langchain import hub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "LOCAL = os.getenv('LOCAL')\n",
    "SAVE_DIR = os.getenv('SAVE_DIR')\n",
    "CHROMA_PATH = os.getenv('CHROMA_PATH')\n",
    "WHISPER_MODEL = os.getenv('WHISPER_MODEL')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:02:42.943849Z",
     "start_time": "2024-06-04T14:02:42.941314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=FgzM3zpZ55o\",\n",
    "    \"https://www.youtube.com/watch?v=E3f2Camj0Is\",\n",
    "    \"https://www.youtube.com/watch?v=dRIhrn8cc9w\",\n",
    "]"
   ],
   "id": "fd5550adc26e9ef8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:02:44.475158Z",
     "start_time": "2024-06-04T14:02:44.463425Z"
    }
   },
   "cell_type": "code",
   "source": "from scripts.rag import vector_store as vs",
   "id": "51019d256968105b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:11:54.294950Z",
     "start_time": "2024-06-04T14:02:45.745155Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store = vs.get_vector_store(urls)",
   "id": "ee27462cecf2802b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following model:  openai/whisper-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=FgzM3zpZ55o\n",
      "[youtube] FgzM3zpZ55o: Downloading webpage\n",
      "[youtube] FgzM3zpZ55o: Downloading ios player API JSON\n",
      "[youtube] FgzM3zpZ55o: Downloading m3u8 information\n",
      "[info] FgzM3zpZ55o: Downloading 1 format(s): 140\n",
      "[download] Destination: ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a\n",
      "[download] 100% of   61.01MiB in 00:00:02 at 21.54MiB/s    \n",
      "[FixupM4a] Correcting container of \"../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a\"\n",
      "[ExtractAudio] Not converting audio ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=E3f2Camj0Is\n",
      "[youtube] E3f2Camj0Is: Downloading webpage\n",
      "[youtube] E3f2Camj0Is: Downloading ios player API JSON\n",
      "[youtube] E3f2Camj0Is: Downloading m3u8 information\n",
      "[info] E3f2Camj0Is: Downloading 1 format(s): 140\n",
      "[download] Destination: ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 2 - Given a Model of the World.m4a\n",
      "[download] 100% of   68.15MiB in 00:00:01 at 40.21MiB/s    \n",
      "[FixupM4a] Correcting container of \"../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 2 - Given a Model of the World.m4a\"\n",
      "[ExtractAudio] Not converting audio ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 2 - Given a Model of the World.m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=dRIhrn8cc9w\n",
      "[youtube] dRIhrn8cc9w: Downloading webpage\n",
      "[youtube] dRIhrn8cc9w: Downloading ios player API JSON\n",
      "[youtube] dRIhrn8cc9w: Downloading m3u8 information\n",
      "[info] dRIhrn8cc9w: Downloading 1 format(s): 140\n",
      "[download] Destination: ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 3 -  Model-Free Policy Evaluation.m4a\n",
      "[download] 100% of   67.74MiB in 00:00:03 at 19.44MiB/s    \n",
      "[FixupM4a] Correcting container of \"../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 3 -  Model-Free Policy Evaluation.m4a\"\n",
      "[ExtractAudio] Not converting audio ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 3 -  Model-Free Policy Evaluation.m4a; file is already in target format m4a\n",
      "Transcribing part ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 3 -  Model-Free Policy Evaluation.m4a!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing part ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a!\n",
      "Transcribing part ../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 2 - Given a Model of the World.m4a!\n",
      "Loaded 3 documents\n",
      "Load videos done !\n",
      "Number of chunks: 142, for 3 documents\n",
      "Split documents done !\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:12:50.233044Z",
     "start_time": "2024-06-04T14:12:50.228486Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vector_store.as_retriever()",
   "id": "905fbdb6c8c48b03",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:13:47.476879Z",
     "start_time": "2024-06-04T14:13:41.478699Z"
    }
   },
   "cell_type": "code",
   "source": "prompt = hub.pull(\"rlm/rag-prompt\")",
   "id": "5d093503d9af2910",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:14:34.300569Z",
     "start_time": "2024-06-04T14:14:34.228346Z"
    }
   },
   "cell_type": "code",
   "source": "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)",
   "id": "826c1742254e2453",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:15:05.335701Z",
     "start_time": "2024-06-04T14:15:05.330779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ],
   "id": "fccdce32a387a665",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:15:26.748047Z",
     "start_time": "2024-06-04T14:15:26.745068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "7a98136e0aa2373e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:18:02.152611Z",
     "start_time": "2024-06-04T14:17:59.706926Z"
    }
   },
   "cell_type": "code",
   "source": "rag_chain.invoke(\"Can you explain me what's the sequential decision making?\")",
   "id": "db777fdea1ac0ecf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sequential decision making is a process where an agent takes actions that affect the state of the world, receives observations and rewards, and aims to maximize future rewards. It involves making decisions based on partially observable information, such as in poker or healthcare scenarios. Types of sequential decision-making processes include bandits and planning, which involve dealing with delayed consequences and exploring the environment to learn how to make optimal decisions.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:18:24.255796Z",
     "start_time": "2024-06-04T14:18:24.251724Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_core.runnables import RunnableParallel",
   "id": "4bb74e2d23c56dc8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:18:33.048025Z",
     "start_time": "2024-06-04T14:18:33.041983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "17ec0a1e5016c78d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:18:39.284084Z",
     "start_time": "2024-06-04T14:18:39.281534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ],
   "id": "8fba662ffad47512",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:18:53.456872Z",
     "start_time": "2024-06-04T14:18:49.951017Z"
    }
   },
   "cell_type": "code",
   "source": "rag_chain_with_source.invoke(\"Can you explain me what's the sequential decision making?\")",
   "id": "a9012038a66df20c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content=\"or sensitive manner, in which case, of course, please feel free to reach out to the course staff directly. And for things like lectures and homeworks and project questions, pretty much all of that should go through Piazza. For late day policy, we have six late days. details you can see the webpage and for collaboration, please see the webpage for some of the details about that. So before we go on to the next part, then we have any questions about logistics for the class. Okay, let's get started. So we're now going to do an introduction to sequential decision making under uncertainty. A number of you guys will have seen some of this content before. We will be going into this in probably more depth than you've seen for some of this stuff including some theory, not theory today, but in other lectures. And then we'll also be moving on to content that will be new to all of you later in the class. So sequential decision making under uncertainty. The fundamental that we think about in these settings is sort of an interactive closed loop process where we have some agent and intelligent agent hopefully that is taking actions that are affecting the state of the world and then it's getting back an observation and a reward. The key goal is that the agent is trying to maximize the total expected future reward. Now, this expected aspect is going to be important because on the other hand, the world itself will be stochastic. And so the agent is going to be maximizing things and\", metadata={'source': '../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a'}),\n",
       "  Document(page_content=\"is treating the world as partially observable, then it says the agent state is not the same and it sort of uses things like the history or beliefs about the world state to aggregate the sequence of previous actions taken and the observations are saved and uses that to make its decisions. For example, in something like poker, you get to see your own cards. Other people have cards that are fully affecting the course of the game. But you don't necessarily know what those are. You can see which cards are discarded. And so that's somewhere where it's naturally partially observable. And so you can maintain a belief state over what the other cards are at the other players. And you can use that information or make your decisions. And similarly often in health care, there's a whole bunch of really complicated physiological processes that are going on, but you can monitor parts of them for things like blood pressure, temperature, et cetera, and then use that in order to make decisions. So in terms of types of sequential decision-making processes, one of them is bandits. We'll talk more about this later at the term. Bandits is sort of a really simple version of a Markov decision process. In this sense that the ideas that the actions that are taken have no influence over the next observation. So when might this be reasonable? So let's imagine that you have a series of customers coming to your website. And you show each of them an ad. And then they either click on it or not. And then you\", metadata={'source': '../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a'}),\n",
       "  Document(page_content=\"move to a new town. You go to there's only one restaurant. You go there the first day and they have five different dishes. You're going to be there for a long time and you want to optimize and get the best dish. And so maybe the first day you try dish one and the second day you try dish two and then the third day three and then et cetera so that you can try everything and then use that to figure out which one is best so that over the long term you pick something that is really delicious. So in this case, the agent has to think explicitly about what decisions it should fake so it can get the information it needs so that in the future it can make good decisions. So in the case of planning and the fact that this is already a hard problem, if you think about something like solitaire, you could already know the rules of the game. It's also two things like go or chats or many other scenarios. And you could know if you take an action, what would be the probability distribution of the next state, and you could use this to compute a potential score. And so using things like tree search or dynamic programming, and we'll talk a lot more about these, particularly the dynamic programming aspect you can use as to decide, given a model of the world, what is the right decision to make? But we're in force about learning itself is a little bit more like solitaire without a rulebook. We're just playing things and you're observing what is happening and you you're trying to get a larger word.\", metadata={'source': '../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a'}),\n",
       "  Document(page_content=\"able to get decisions that are good. The second situation is delayed consequences. So this is the challenge that the decisions that are made now. You might not realize whether or not they're a good decision until much later. So you eat the chocolate sunday now and you don't realize until an hour later that that was a bad idea to eat all two courts of ice cream. Or you, in the case of things like video games like Montezuma's revenge, you have to pick up a key and then much later you realize that's helpful. Or you study really hard now on Friday night and in three weeks you do well on the midterm. So one of the challenges to doing this is that because you don't necessarily receive immediate outcome feedback, it can be hard to do what is known as the credit assignment problem, which is how do you figure out the causal relationship between the decisions you made in the past and the outcomes in the future? And that's a really different problem than we tend to see in most of machine learning. So one of the things that comes up when we start to think about this is how do we do exploration? So the agent is fundamentally trying to figure out how the world works through experience in much of reinforcement learning. And so we think about the agent is really kind of being the scientist of trying things out in the world, like having an agent that tries to rewrite a bicycle and then learning about health physics and writing it about what's like, good works by falling. Now one of the\", metadata={'source': '../../data/youtube/Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a'})],\n",
       " 'question': \"Can you explain me what's the sequential decision making?\",\n",
       " 'answer': 'Sequential decision making is a process where an agent takes actions that affect the state of the world, receives observations and rewards, and aims to maximize future rewards. It involves making decisions based on partially observable information, such as in poker or healthcare scenarios. Types of sequential decision-making processes include bandits, which are simpler versions of Markov decision processes where actions have no influence on future observations.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
