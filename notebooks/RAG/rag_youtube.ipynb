{
 "cells": [
  {
   "cell_type": "code",
   "id": "d9d4219514005b4b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T14:02:23.857451Z",
     "start_time": "2024-06-04T14:02:23.851564Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParserLocal\n",
    "from typing import List\n",
    "from langchain.schema import Document"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:02:29.126056Z",
     "start_time": "2024-06-04T14:02:23.871327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "urls = [\"https://www.youtube.com/watch?v=FgzM3zpZ55o\",\n",
    "        \"https://www.youtube.com/watch?v=E3f2Camj0Is\",\n",
    "        \"https://www.youtube.com/watch?v=dRIhrn8cc9w\",\n",
    "        \"https://www.youtube.com/watch?v=j080VBVGkfQ\",\n",
    "        \"https://www.youtube.com/watch?v=buptHUzDKcE\",\n",
    "        \"https://www.youtube.com/watch?v=gOV8-bC1_KU\",\n",
    "        \"https://www.youtube.com/watch?v=V7CY68zH6ps\",\n",
    "        \"https://www.youtube.com/watch?v=8LEuyYXGQjU\",\n",
    "        \"https://www.youtube.com/watch?v=E-_ecpD5PkE\",\n",
    "        \"https://www.youtube.com/watch?v=o_i5F1zGPLs\",\n",
    "        \"https://www.youtube.com/watch?v=RN8qpSs8ozY\",\n",
    "        \"https://www.youtube.com/watch?v=jJ7JbQBTChM\",\n",
    "        \"https://www.youtube.com/watch?v=Hg_uyWezMM0\",\n",
    "        \"https://www.youtube.com/watch?v=zPU1SRHuAW8\",\n",
    "        \"https://www.youtube.com/watch?v=vDF1BYWhqL8\"]\n",
    "\n",
    "save_dir = \"../../data/audio/youtube/\"\n",
    "\n",
    "loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal(lang_model=\"openai/whisper-tiny\"))\n",
    "\n",
    "docs = loader.load()\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following model:  openai/whisper-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=FgzM3zpZ55o\n",
      "[youtube] FgzM3zpZ55o: Downloading webpage\n",
      "[youtube] FgzM3zpZ55o: Downloading ios player API JSON\n",
      "[youtube] FgzM3zpZ55o: Downloading m3u8 information\n",
      "[info] FgzM3zpZ55o: Downloading 1 format(s): 140\n",
      "[download] Destination: ../../data/audio/youtube//Stanford CS234： Reinforcement Learning ｜ Winter 2019 ｜ Lecture 1 - Introduction - Emma Brunskill.m4a\n",
      "[download]  49.5% of   61.01MiB at   19.51MiB/s ETA 00:01  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x707e81b87370>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matthieu/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 21\u001B[0m\n\u001B[1;32m     17\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../data/audio/youtube/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m loader \u001B[38;5;241m=\u001B[39m GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal(lang_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai/whisper-tiny\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m---> 21\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[43mloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:29\u001B[0m, in \u001B[0;36mBaseLoader.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlazy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/generic.py:115\u001B[0m, in \u001B[0;36mGenericLoader.lazy_load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlazy_load\u001B[39m(\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    113\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Document]:\n\u001B[1;32m    114\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m blob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblob_loader\u001B[38;5;241m.\u001B[39myield_blobs():  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblob_parser\u001B[38;5;241m.\u001B[39mlazy_parse(blob)\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/blob_loaders/youtube_audio.py:45\u001B[0m, in \u001B[0;36mYoutubeAudioLoader.yield_blobs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murls:\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;66;03m# Download file\u001B[39;00m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m yt_dlp\u001B[38;5;241m.\u001B[39mYoutubeDL(ydl_opts) \u001B[38;5;28;01mas\u001B[39;00m ydl:\n\u001B[0;32m---> 45\u001B[0m         \u001B[43mydl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Yield the written blobs\u001B[39;00m\n\u001B[1;32m     48\u001B[0m loader \u001B[38;5;241m=\u001B[39m FileSystemBlobLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_dir, glob\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.m4a\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:3583\u001B[0m, in \u001B[0;36mYoutubeDL.download\u001B[0;34m(self, url_list)\u001B[0m\n\u001B[1;32m   3580\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SameFileError(outtmpl)\n\u001B[1;32m   3582\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m url_list:\n\u001B[0;32m-> 3583\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__download_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_info\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3584\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_generic_extractor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforce_generic_extractor\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_retcode\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:3558\u001B[0m, in \u001B[0;36mYoutubeDL.__download_wrapper.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   3555\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m   3556\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   3557\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3558\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3559\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m UnavailableVideoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   3560\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreport_error(e)\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:1595\u001B[0m, in \u001B[0;36mYoutubeDL.extract_info\u001B[0;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001B[0m\n\u001B[1;32m   1593\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m ExistingVideoReached()\n\u001B[1;32m   1594\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1595\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__extract_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_info_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocess\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1596\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1597\u001B[0m     extractors_restricted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mallowed_extractors\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:1606\u001B[0m, in \u001B[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1604\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1605\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1606\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1607\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (DownloadCancelled, LazyList\u001B[38;5;241m.\u001B[39mIndexError, PagedList\u001B[38;5;241m.\u001B[39mIndexError):\n\u001B[1;32m   1608\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:1762\u001B[0m, in \u001B[0;36mYoutubeDL.__extract_info\u001B[0;34m(self, url, ie, download, extra_info, process)\u001B[0m\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m process:\n\u001B[1;32m   1761\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_video(ie_result)\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_ie_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mie_result\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_info\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ie_result\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:1821\u001B[0m, in \u001B[0;36mYoutubeDL.process_ie_result\u001B[0;34m(self, ie_result, download, extra_info)\u001B[0m\n\u001B[1;32m   1819\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvideo\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_extra_info(ie_result, extra_info)\n\u001B[0;32m-> 1821\u001B[0m     ie_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_video_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mie_result\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1822\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_pending_errors(ie_result)\n\u001B[1;32m   1823\u001B[0m     additional_urls \u001B[38;5;241m=\u001B[39m (ie_result \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madditional_urls\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:2993\u001B[0m, in \u001B[0;36mYoutubeDL.process_video_result\u001B[0;34m(self, info_dict, download)\u001B[0m\n\u001B[1;32m   2991\u001B[0m downloaded_formats\u001B[38;5;241m.\u001B[39mappend(new_info)\n\u001B[1;32m   2992\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2993\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_info\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2994\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MaxDownloadsReached:\n\u001B[1;32m   2995\u001B[0m     max_downloads_reached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:3459\u001B[0m, in \u001B[0;36mYoutubeDL.process_info\u001B[0;34m(self, info_dict)\u001B[0m\n\u001B[1;32m   3455\u001B[0m dl_filename \u001B[38;5;241m=\u001B[39m existing_video_file(full_filename, temp_filename)\n\u001B[1;32m   3456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dl_filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m dl_filename \u001B[38;5;241m==\u001B[39m temp_filename:\n\u001B[1;32m   3457\u001B[0m     \u001B[38;5;66;03m# dl_filename == temp_filename could mean that the file was partially downloaded with --no-part.\u001B[39;00m\n\u001B[1;32m   3458\u001B[0m     \u001B[38;5;66;03m# So we should try to resume the download\u001B[39;00m\n\u001B[0;32m-> 3459\u001B[0m     success, real_download \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemp_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3460\u001B[0m     info_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__real_download\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m real_download\n\u001B[1;32m   3461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/YoutubeDL.py:3180\u001B[0m, in \u001B[0;36mYoutubeDL.dl\u001B[0;34m(self, name, info, subtitle, test)\u001B[0m\n\u001B[1;32m   3178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_headers\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3179\u001B[0m     new_info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_headers\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calc_headers(new_info)\n\u001B[0;32m-> 3180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubtitle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/downloader/common.py:466\u001B[0m, in \u001B[0;36mFileDownloader.download\u001B[0;34m(self, filename, info_dict, subtitle)\u001B[0m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_screen(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[download] Sleeping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep_interval\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds ...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    464\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(sleep_interval)\n\u001B[0;32m--> 466\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreal_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finish_multiline_status()\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/downloader/http.py:370\u001B[0m, in \u001B[0;36mHttpFD.real_download\u001B[0;34m(self, filename, info_dict)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m     establish_connection()\n\u001B[0;32m--> 370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RetryDownload \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    372\u001B[0m     retry\u001B[38;5;241m.\u001B[39merror \u001B[38;5;241m=\u001B[39m err\u001B[38;5;241m.\u001B[39msource_error\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/downloader/http.py:252\u001B[0m, in \u001B[0;36mHttpFD.real_download.<locals>.download\u001B[0;34m()\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;66;03m# Download and write\u001B[39;00m\n\u001B[0;32m--> 252\u001B[0m         data_block \u001B[38;5;241m=\u001B[39m \u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_test\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mblock_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_len\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbyte_counter\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TransportError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    254\u001B[0m         retry(err)\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/yt_dlp/networking/_requests.py:141\u001B[0m, in \u001B[0;36mRequestsResponseAdapter.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread\u001B[39m(\u001B[38;5;28mself\u001B[39m, amt: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    140\u001B[0m         \u001B[38;5;66;03m# Interact with urllib3 response directly.\u001B[39;00m\n\u001B[0;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# See urllib3.response.HTTPResponse.read() for exceptions raised on read\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m urllib3\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mSSLError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/urllib3/response.py:935\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[1;32m    933\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[0;32m--> 935\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    937\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[1;32m    939\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/urllib3/response.py:862\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    859\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 862\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[1;32m    865\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    870\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/UbuntuData/PycharmProjects/snoop/.venv/lib/python3.10/site-packages/urllib3/response.py:845\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    842\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1()\n\u001B[1;32m    843\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    844\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 845\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    465\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 466\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/usr/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/ssl.py:1303\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1300\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1301\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1302\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1304\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/usr/lib/python3.10/ssl.py:1159\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1159\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1160\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1161\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:16:08.462376Z",
     "start_time": "2024-06-04T14:16:08.459104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(docs))\n",
    "\n",
    "# Print keys of docs\n",
    "print(docs[10].page_content)"
   ],
   "id": "948d769b97e5633c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      " Hi everybody, I'm Emma Brentskill. I'm an Assistant Professor in Computer Science and welcome to CS 234, which is reinforcement learning class, which is designed to be sort of an entry level master's or PhD student in an introduction to reinforcement learning. So what we're going to do today is I'm going to start with just a really short brief overview of what is reinforcement learning. And then we're going to go through course logistics. And when I go through course logistics, I'll also pause and ask for any questions about logistics. The website is now live and so that's also the best source of information about the class. That MPAZ will be the best source of information. So I'll stop there. We'll be get to that part to ask if there's anything that don't go over that you have questions about. And if you have questions about the wait list or any particular things relating to your own circumstance feel free to come up to me at the end. And then the third part of the class is going to be where we're starting to get into the technical content of we thinking about an introduction to sequential decision making under uncertainty. Just so I have a sense before we get started, who here has taken a machine learning class? All right, who here has taken AI? Okay, so a little bit less but most people. All right, great. So probably everybody here has seen a little bit about reinforcement learning. There is a little bit to put in on where you've been at. We will be covering stuff starting in the beginning as if you don't know any reinforcement learning. Um, but then we'll rapidly be getting to other content. Um, that's beyond anything that's covered in at least other Stanford related classes. So reinforcement learning is concerned with this really foundational issue of how can intelligent agent learn to make a good sequence of decisions. And that's sort of a single sentence that summarizes what reinforcement learning is doing and what we'll recover in during this class. But it actually encodes a lot of really important ideas. So the first thing is that we're really concerned now with sequences of decisions. So contrast to a lot of what is covered in machine learning, we're going to be thinking about agents, intelligent agents, or an intelligent agent in general, that might or might not be human or biological, and how it can make not just one decision but a whole sequence of decisions. We're going to be concerned with goodness. In other words, we're going to be interested in the second thing is how do we learn to make good decisions? And what we mean by good here is some notion of optimality. We have some utility measure over the decisions that are being made. And the final critical aspect of reinforcement learning is the learning that the agent doesn't know in advance how its decisions are going to affect the world or what decisions might necessarily be associated with good outcomes and instead it has to acquire that information through experience. So when we think about this, this is really something that we do all the time. We've done it since we were babies. We try to figure out how to sort of achieve high reward in the world and there's a lot of really exciting work that's going not in neuroscience and psychology. That's trying to think about the same fundamental issue from the perspective of human intelligent agents. And so I think that if we want to be able to solve AI or make significant progress, we have to be able to make significant progress in allowing us to create agents that do reinforcement learning. So where does this come up? There's this nice example from Yale Knit, who's an amazing student psychologist and neuroscience researcher at Princeton, where she gives this example of this primitive creature, which evolves as following during its lifetime. So when it's a baby, it has a primitive brain and one eye, and it swims around and it attaches to a rock. And then what is an adult? It digests its brain and it sits there. And so maybe this is some indication that the point of intelligence or the point of having a brain, in at least in part, is helping to guide decisions. And so that once all the decisions in the agents life has been completed, maybe we no longer need a brain. So I think this is one example of a biological creature, but I think it's a useful reminder to think about why would an agent need to be intelligent and is it somehow fundamentally related to the fact that it has to make decisions? Now of course, there's been a sort of really a paradigm shift in reinforcement learning around 2015 in the NERIPS conference, which is one of the main machine learning conferences. David Silver came and went to a workshop and presented these incredible results of using reinforcement learning to directly control Atari games. Now these are important in whether you like video games or not. Video games are really interesting examples of complex tasks that take human players a while off until learn. We don't know how to do them in advance. It takes us at least a little bit of experience. And what the really incredible thing about this example was, this is breakout, is that the agent learns to play directly from pixel input. So from the agents perspective, they're just seeing sort of these colored pixels coming in and it's having to learn what's the right decisions to make in order to learn to play the game well and in fact even better than people. So this was really incredible that this was possible. When I first started doing reinforcement learning, a lot of the work was really focused on very artificial toy problems. A lot of the foundations were there, but these sort of larger scale applications were really lacking. And so I think in the last five years we've seen really a huge improvement in the types of techniques that are going on in reinforcement learning and in the scale of the problems that can be tackled. Now it's not just in video game playing, it's also in things like robotics. And particularly some of my colleagues up at University of California, Berkeley. I have been doing some really incredible work on robotics and using reinforcement learning in these types of scenarios to try to have agents do grasping, fold clothes, things like that. Now, those sort of examples, if you guys have looked at reinforcement learning before, are probably the ones you've heard about. You've probably heard about things like video games or robotics. But one of the things that I think is really exciting is that reinforcement learning is actually applicable to a huge number of domains, which is both an opportunity and a responsibility. So in particular, I direct the AI for Human Impact Lab here at Stanford. And one of the things that we're really interested in is how do we use artificial intelligence to amplify human potential? So what way you can imagine doing that is through something like educational games, where the goal is to figure out how to quickly and effectively teach people how to learn material such as fractions. Another really important application area is healthcare. This is sort of a cutout of looking at seizures that some work that's been done by Joel Pinot, up at McGill University. And I think there's also a lot of excitement right now thinking about how can we use AI in a particular reinforcement learning to interact with things like electronic medical record systems and use them to inform patient treatment. There's also a lot of recent excitement in thinking about how we can use reinforcement learning and lots of other applications kind of as an optimization technique for when it's really hard to solve optimization problems. So this is a rising in things like natural language processing and vision and a number of other areas. So I think if we have to think about what are the key aspects of reinforcement learning, they probably boil down to the following four. And these are things that are going to distinguish it from other aspects of AI machine learning. So reinforcement learning from my sentence about that we're learning to make good decisions under uncertainty, fundamental involves optimization, delayed consequences, exploration, and generalization. So optimization actually comes up because we're introducing good decisions. There's some notion of relative different types of decisions that we can make and we want to be able to get decisions that are good. The second situation is delayed consequences. So this is the challenge that the decisions that are made now. You might not realize whether or not they're a good decision until much later. So you eat the chocolate sunday now and you don't realize until an hour later that that was a bad idea to eat all two courts of ice cream. Or you, in the case of things like video games like Montezuma's revenge, you have to pick up a key and then much later you realize that's helpful. Or you study really hard now on Friday night and in three weeks you do well on the midterm. So one of the challenges to doing this is that because you don't necessarily receive immediate outcome feedback, it can be hard to do what is known as the credit assignment problem, which is how do you figure out the causal relationship between the decisions you made in the past and the outcomes in the future? And that's a really different problem than we tend to see in most of machine learning. So one of the things that comes up when we start to think about this is how do we do exploration? So the agent is fundamentally trying to figure out how the world works through experience in much of reinforcement learning. And so we think about the agent is really kind of being the scientist of trying things out in the world, like having an agent that tries to rewrite a bicycle and then learning about health physics and writing it about what's like, good works by falling. Now one of the really big challenges here is that data is censored. And what we mean by censoring in this case is that you only get to learn about what you try to do. So all of you guys are here at Stanford. Clearly that was the optimal choice. But you don't actually get to figure out what it would have been like if you went to MIT. It's possible that would have been a good choice as well. But you can't experience that because you only get to live one life. And so you only get to see the particular choice you met at this particular time. So one question you might wonder about is, you know, a policy, what we're gonna talk a lot about policies, policies, decision policies is gonna be some mapping from experiences to a decision. And you might answer why this needs to be learned. So if we think about something like deep minds, a tarry playing game, what it was learning from here is it was learning from pixels. So it was essentially learning from the space of images what to do next. And if you wanted to write that down as a program a series of if-then statements it would be absolutely enormous. This is not tractable. So this is why we need some form of generalization. Why it may be much better for us to learn from data directly as well as to have some high-level representation of the task, so that even if we then run into a particular configuration of pixels we've never seen before, our agent can still know what to do. So these are the four things that really make up reinforcement learning, at least online reinforcement learning, and why are they different than some other types of AI machine learning? So another thing that comes up a lot in artificial intelligence is planning. So for example, the go game is can be thought of as a planning problem. So what does planning evolve? It evolves optimization, often generalization, and delayed consequences. You might take a move and go early, and it might not be immediately obvious if that was a good move until many steps later, but it doesn't involve exploration. The idea in planning is that you're given a model of how the world works. So you're given the rules of the game, for example, and you know what the reward is, and that hard part is computing what you should do given the model of the world. So it doesn't require exploration. In supervised machine learning versus reinforcement learning, it often involves optimization and generalization, but frequently it doesn't involve either exploration or delayed consequences. So it doesn't tend to involve exploration because typically in supervised learning, you're given a dataset. So your agent isn't collecting its experience or data about the world instead of given an experience and that has to use that to say in for whether an image is a face or not. Similarly, it's typically making essentially one decision. Whether this image is a face or not, instead of having to think about making decisions now and then only learning whether or not those the right decisions later. Unsupervised machine learning also involves optimization in generalization, but generally does not involve exploration or delayed consequences, and typically you have no labels for about the world. So in supervised learning, you often get the exact label for the world. Like, this image really is, contains a face or not. In unsupervised learning, you normally get no labels about the world. And in RL, you typically get something kind of halfway in between those, which you get a utility of the label you put. So, for example, you might decide that there's a face in here and it might say, okay, yeah, we'll give you partial credit for that. Because maybe there's something that looks sort of like a face. But you don't get the true label of the world or maybe you decide to go to Stanford and then you don't know, and you're like, okay, that was a really great experience, but I don't know if it was quote unquote the right experience. Invitational learning, which is something that will probably touch on briefly in this class and is becoming very important, is similar, but a little bit different. So it involves optimization, generalization, and often delayed consequences, but the idea is that we're going to be learning from experience of others. So instead of our intelligent agent getting to take experiences from the world of make its own decisions, you might watch another intelligent agent, which might be a person, make decisions, observe outcomes, and then use that experience to figure out how it wants to act. It could be a lot of benefits to doing this, but it's a little bit different because it doesn't have going to talk about the exploration problem. where he was looking to expert flights together with Peter Abiel, who's a professor over at Berkeley, to see how you could imitate very quickly experts flying toy helicopters. That was one of sort of the first kind of major applications successes of invitation learning. It can be very effective. There can be some challenges to it because essentially if you get to observe one trajectory, let's imagine it's a circle of a helicopter flying, and your agent learns something that isn't exactly the same as what the expert was doing, that you can essentially start to go off that path and venture into territory where you really don't know what the right thing is to do. So there's been a lot of extensive work on a mutation learning that's sort of combining between a mutation learning and reinforcement learning that ends up being very promising. So in terms of how we think about trying to do reinforcement learning, we can build on a lot of these different types of techniques. And then also think about some of the challenges that are unique to reinforcement learning, which involves all four of these challenges. And so these are all agents really need to explore the world and then use that exploration to guide their future decisions. So we'll talk more about this throughout the course. A really important question that comes up with weird of these rewards come from. Where is this information that the agents are using to try to guide whether or not their decisions are good and who is providing those and what happens if they're wrong? And we'll talk a lot more about that. We won't talk very much about multi-agent reinforcement learning systems, but that's also a really important case, as well as thinking about the game theoretic aspects. All right, so that's just sort of a really short overview about some of the aspects of reinforcement learning and why it's different than some of the other classes that you might have taken. And now we're going to go briefly through course logistics and then start to move more of the content and I'll pause after course logistics. That's for any questions. In terms of prerequisites, we expect that everybody here has either taken an AI class or a machine learning class, either here at Stanford or the equivalent in another institution. If you're not sure whether or not you have the right background for the class, feel free to reach out to us on Piazza and we'll respond if you've done extensive work and related stuff, it'll probably be sufficient. In general, we expect that you have basic Python proficiency and that you're familiar with probabilities to statistics and mult So this is a long list. I'll go through it slowly because I think it's pretty important. So this is what are the goals for the class? What are the learning objectives? So these are the things that we expect that you guys should be able to do by the time you finish this class and that it's our role to help you be able to understand how to do these things. So the first thing is that it's important to be able to define the key features of reinforcement learning that distinguish it from other types of AI machine learning frames of problems. So that's what I was doing a little bit of so far in this class to figure out how does this distinguish this? How does our all distinguish itself from other types of problems? So related to that, for most of you, you will probably not end up being academics and most of you will go into industry. And so one of the big challenges when you do that is that when you're faced with a particular problem from your boss or when you're giving a problem to one of your supervisees, is for them to think about whether or not it should be framed as a reinforcement learning problem. And what things are applicable to it. So I think it's very important by the end of this class that you have a sense of if you're given a real-world problem like web advertising or patient treatment or robotics problem that you have a sense whether or not it is useful to formulate it as a reinforcement learning problem and how to write it down in that framework and what algorithms are relevant. During the class, we'll also be introducing you to a number of reinforcement learning algorithms and you will have the chance to implement those in code, including deep reinforcement learning problems. Another really important aspect is if you're trying to decide what tools to use for a particular save robotics problem or healthcare problem, is to understand which of the algorithms is likely to be beneficial and why. And so in addition to things like empirical performance, I think it's really important to understand generally how do we evaluate algorithms? And can we use things like theoretical tools, like regret, sample complexity, as well as things like computational complexity to decide which algorithms are suitable for particular tasks? And then the final thing is that one really important aspect of reinforcement learning is exploration versus exploitation. This issue that arises with an agents have to figure out what decisions they want to make and what they're going to learn about the environment by making those decisions. And so by the end of the class, you should also be able to compare different types of techniques for doing exploration versus exploitation and what are the strengths and limitations of these. Does there may have any questions about what these learning objectives are? Okay. So we'll have three main assignments for the class. We'll also have a midterm. I will have a quiz at the end of the class as well as a final project. The quiz is a little bit unusual. So I just want to spend a little bit of time talking about it right now. The quiz is done both individually and in groups. The reason that we do this is because we want a low stakes way to sort of have people practice with the material that they learn in the second half of the course in a way that sort of fun engaging and really tries to get you to think about it and also learn from your peers. And so we did it last year and I think a number of people were a little bit nervous about how it would go before and then ended up really enjoying it. So the way that the quiz works is it's a multiple choice quiz. At the beginning, everybody does it by themselves. And then after everybody has submitted their answers, then we do it again in groups that are pre-assigned by us. And the goal is that you have to get everyone to decide on what the right answer is before you scratch off and see what the correct answer is. And then we grade it according to what you scratched off the right answer correctly first or not. You can't do worse than your individual grade. So, the doing it in a group can only help you. And for SEPD students, they don't do it in groups. So they just write down justifications for their answers. Again, it's a pretty lightweight way to do assessment. The goal is that you sort of have to be able to articulate why you believe the answers are the way they are and discuss them in small groups and they use that to figure out what the correct answer is. The final project is pretty similar to other projects that you guys have done in other classes. It's an open end to project. It's a chance to reason about and think about reinforcement learning stuff in more depth. We will also be offering a default project that will be announced over the next couple of weeks before the first milestone is due. If you choose to do the default project, you're breakdown because you will not need to do a proposal or milestone. We'll be based on the project presentation in your assignment. Right up. Since we believe that you guys are all of each other's best resource, we use Piazza. That should be used for pretty much all class communication unless it's something of sort of a private or sensitive manner, in which case, of course, please feel free to reach out to the course staff directly. And for things like lectures and homeworks and project questions, pretty much all of that should go through Piazza. For late day policy, we have six late days. details you can see the webpage and for collaboration, please see the webpage for some of the details about that. So before we go on to the next part, then we have any questions about logistics for the class. Okay, let's get started. So we're now going to do an introduction to sequential decision making under uncertainty. A number of you guys will have seen some of this content before. We will be going into this in probably more depth than you've seen for some of this stuff including some theory, not theory today, but in other lectures. And then we'll also be moving on to content that will be new to all of you later in the class. So sequential decision making under uncertainty. The fundamental that we think about in these settings is sort of an interactive closed loop process where we have some agent and intelligent agent hopefully that is taking actions that are affecting the state of the world and then it's getting back an observation and a reward. The key goal is that the agent is trying to maximize the total expected future reward. Now, this expected aspect is going to be important because on the other hand, the world itself will be stochastic. And so the agent is going to be maximizing things and expectation. This may not always be the right criteria. This has been what has been focused on for the majority of reinforcement learning, but there's now some interest in thinking about distribution arable, RL and some other aspects. One of the key challenges here is that it can require balancing between immediate and long-term rewards, and that it might require strategic behavior in order to achieve those high rewards. Indicating that you might have to sacrifice initial higher rewards in order to achieve better rewards over the long-term. So as an example, something like web advertising might be that you have a agent that is running the website, and it has to choose which web ad to give to a customer. The customer gives you back an observation such as how long they spent on the web page and also you get some information about whether or not they click on an add. And the goal is to say have people click on adds the most. So you have to pick which add to show people so that they're going to click on adds. Another example is a robot that's unloading a dishwasher. So in this case, the action space of the agent might be joint movements. And the information the agent might get back was a camera image of the kitchen. And it might get a plus one reward if there are no dishes on the counter. So in this case, it would generally be a delayed reward for a long time they're going to be dishes on the counter unless it can just sweep all of them off and have them crash onto the floor, which may or may not be the intended goal of the person who's writing this system. And so it may have to make a sequence of the decisions where it can't get any reward for a long time. Another example is something like blood pressure control where the actions might be things like prescribed exercise or prescribed medication and we get an observation back of what is the blood pressure of the individual and then the reward might be plus one if it's in the if the blood pressure is in a healthy range maybe a small negative reward if medication is prescribed due to side effects it it may be zero or word otherwise. Okay, so let's think about another case, like some of the cases that I think about in my lab, like having no artificial tutor. So now what you could have is you could have a teaching agent. And what it gets to do is pick an activity. So pick a teaching activity. And let's say it only has two different types of teaching activities to get. It's going to either give an addition activity or subtraction activity. And it gives this to a student. And then the student either gets the problem right or wrong. And let's say the student initially does not know addition or subtraction. So it's a kindergarten error that student doesn't know anything about math and we're trying to figure out how to teach the student at math. And the reward structure for the teaching agent is they get a plus one every time the student gets something right and they get a minus one if the student gets it wrong. So I'd like you to just take a minute, turn to somebody nearby, and describe what you think an agent that's trying to learn to maximize its expected rewards would do in this type of case, what type of problems it would give to the student, and whether or not that is doing the right thing. Thank you. And let me just clarify here. And let me just clarify here. And let me just clarify here that let's assume that for most students, addition is easier than subtraction. So that like what it says here that the problem, even though the student doesn't know either of these things that the skill of learning addition is simpler for a new student to learn that subtraction. So what would what might happen under those cases? So then maybe we want to raise their hand and tell me what they and somebody nearby them were thinking might happen for an agent in this scenario. The agent would give them really easy addition problems. That's correct. And that's exactly actually what happened. There's a nice paper from approximately 2000 with Bev Wolfe, which is one of the earliest ones that I know were there using reinforcement learning to create an intelligent tutoring system. And the reward was for the agent to give problems to the student in order to get them correct. Because if the students get anything's correct, then they've learned them. But the problem here is with that reward specification, what the agent learned to do is to give really easy problems. And then maybe the student doesn't know how to do those initially, but then they quickly learn how, and then there's no incentive to give hard problems. So this is just sort of a small example of what is known as reward hacking, which is that your agent is gonna learn to do exactly what it is that you tell them to do in terms of the rewards function that you specify. And yet in reinforcement learning often we spend very little of our time thinking very carefully about what that reward function is. So whenever you get out into sort of the real world, this is the really, really critical part. But normally, it is the designer that gets to pick what the reward function is. The agent is not having intrinsic internal reward. And so depending on how you specify it, the agent will learn to do different things. Yeah, what's the question in the back? Just in this case, it seems like the student would also be in RL agent. And that's like in real life, the student who does the best would like we ask for hard questions and your reward if not to So, so we like techniques to approach that or to create more of that stuff. So, the question was to say, well, you know, we also think that people are probably reinforcement learning agents as well and that's exactly correct and maybe they would start to say, hey, I need to get harder questions or be interactive in this process. For most of this class, we're going to ignore the fact that the world that we're interact with itself might also be an RL agent. In reality, it's really critical. Sometimes this is often considered in an adversarial way, like for game theory. I think one of the most exciting things to me is when we think about it in a cooperative way. So who here's heard about the subdiscipline of machine teaching? Nobody yet. So it's a really interesting new area that's been around for maybe five to ten years. A little bit beyond that. And what the idea is there is what happens if you have two intelligent agents that are interacting with each other, where they know that each other is trying to help them. So there's a really nice, classic example from, sorry for those of you that aren't so familiar with the Schienen-Learning, but imagine that you're trying to learn a classifier to decide where along this line, things are either positive or negative. So in general, you're going to need some amount of samples. That's if you wear that sort of the number of points on the line where you have to get positive or negative labels. If you're in an active learning setting, generally I think you can reduce that to roughly log in. By being strategic about asking people to label particularly points in the line, one of the really cool things for machine teaching is that if I know you are trying to teach me where to divide this line, you only need one point or at most two points. Essentially constant, right? Because if I'm trying to teach you, there's no way I'm just gonna randomly label things. I'm just gonna label you in single plus and that minus and that's gonna tell you exactly where the line goes. So that's one of the reasons why if the agent knows that the other agent is trying to teach them something, it can actually be enormously more efficient than what we normally think of for learning. And so I think there's a lot of potential for machine teaching to be really effective. But all that said, we're going to ignore most of that for the course. If it's something you want to explore in your project, you're very welcome to. There's a lot of connections with reinforcement learning. Okay, so if we think about this process in general, if we think of sort of a sequential decision making process, we have this agent. We're going to think about almost always about there being discrete time. So agent's going to make a decision. It's going to affect the world in some way. It's going to see the world is going to give some new observation and a reward. And the agent receives those and uses it to make another decision. So in this case, when we think about a history, what we mean by history is simply the sequence of previous actions that the agent took and the observations and rewards it received. And then the second thing that's really important is to define a state space. Again, often when this is first discussed, this is sort of thought about as some immutable thing, but whenever you're in a real application, this is exactly what you have to define. It's how to write down the representation of the world. What we're going to assume in this class is that this data is a function of the history. So there might be other aspects of, there might be other sensory information that the agent would like to have access to in order to make its decision. But it's going to be constrained to the observations it's received so far. The actions it's taken and the rewards it's observed. Now, there's also going to be some real world state. So that's the real world. And the agent doesn't necessarily have access to the real world. They may have access only to a small subset of the real world. So for example, as a human right now, I have eyes that allow me to look forward, you know, roughly 180 degrees, but I can't see by my head. But behind my head is still part of the world state. So the world state is the real world, and then the agent has its own state space that uses to try to make decisions. So in general, we're going to assume that there's some function of the history. Now, what assumption that we're going to use a lot in this class, which you guys have probably seen before is the Markov assumption. And the Markov assumption simply says that we're going to assume that the state used by the agent is a sufficient statistic of the history in the in order to predict the future. You only need to know the current state of the environment. So simply basically indicates that the future is independent of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state need to know the current state of the environment. So simply basically indicates that the future is independent of the past, given the present, if in the present you have the right aggregate statistic. So there's a couple examples of this? Yeah. Question, Naman. Can you just explain maybe with an example of a difference again between the state and the history like I'm having trouble with some family. Yeah, so this state, if we think about something like. Or a robot. So let's say you have a robot that is walking down a long corridor. Let's say there's two long corridors. Okay, so your robot starts here. This is where your robot starts. And it tries to go right, right, and then it goes down, down. And let's say it sensors are just that it can observe whether in front of it, whether there is a wall on any of its sides. So the observation space of the robot is simply is there a wall on any side, on each of its four sides. Sorry, that's probably a little bit small in the back. But the agent basically has some sort of local amount of the laser rangefinder or something like that. So it knows whether or not there is a wall immediately around it. That's a room that's square and nothing else. So in this case, what the agent would see is that initially the wall looks like this and then like this and then like this and then like this and the history would include all of this but its local state is just this. So the local state could just be the current observation. And that starts to be important when you're going down here because there are many places that look like that. And so if you keep track of the whole history, the agent can figure out where it is, but if it only keeps track of where it is locally, then a lot of partial aliasing can occur. So I put up a couple examples here. So it's something like hypertension control. You can imagine the state is just the current blood pressure. And your action is whether it would take medication or not. So curbler pressure, meaning like every second, for example, what is your blood pressure? So do you think the sort of system is mark off? I see some people shaking their heads. Almost definitely not. Almost definitely there are other features that I have to do with, you know, maybe whether or not you're exercising, whether or not you just ate a meal, whether it's hot outside, whether if you just got an airplane, all these other features probably affect whether or not your next blood pressure is going to be higher low and particularly in response to some medication. Similarly, something like website shopping, can imagine this state is just sort of what is the product you're looking at right now. So like I open on Amazon, I'm looking at some computer and that's up on my web page right now. And the action is what other products do you recommend? Do you think that system is Markov? System is not Markov. Do you mean the system generally, but that the assumption is Markov? And it doesn't fit. The question is whether or not the system generally is Markov and the assumption doesn't fit or just some more details I think about this. What I mean here is that this particular choice of representing the system is that Markov. And so there's the real world going on and then there's sort of the model of the world that the agent can use and what I'm arguing here is that these particular models of the world are not mark off. There might be other models of the world that are. But if we choose this particular observation, say just the current blood pressure is our state, that is probably not really a mark off state. Now it doesn't mean that we can't use algorithms that treat it as if it is, it's just that we should be aware that we might be violating some of those assumptions. Yeah. So if you include an in-off history you can do a stick and you make a problem in a Mark-off. So great question. So why is this so popular? Can you always make something mark off? Generally, yes, if you include all the history, then you can always make this a star mark off. In practice, often you can get away with just using the most recent observation. Or maybe the last four observations as a reasonably sufficient statistic. It depends a lot on the domain. There's certainly domain, maybe like the navigation world I put up there, where it's really important to model, either use the whole history as the state, or think about the partial observability. And other cases where maybe the current, most recent observation is completely sufficient. Now one of the challenges here is you might not want to use the whole history because that's a lot of information and you have to keep track of it over time and so it's much nicer to have sort of a sufficient statistic. Of course some of these things are changing a little bit with LSTMs and other things like that. So some of our prior assumptions about sort of how things scale with the size of the state space are changing a little bit right now with deep learning. But historically certainly, it has been advantages to having a smaller state space. And again, historically there's been a lot of implications for things like computational complexity, the data required, and the resulting performance, depending on the size of the state space. So just to give some intuition for why that might be, if you made your state everything that's ever happened to in your life, that would give you a really, really rich representation, but you would only have one data point for every state. There would be no repeating. So it's really hard to learn because all states are different. And in general, if we want to learn how to do something, we're going to either need some form of generalization or some form of clustering or aggregation so that we can compare experiences so that we can learn from prior similar experience in order to what to do. So if we think about assuming that your observation is your state, so the most recent observation that the agent gets, we're going to treat that as the state. Then we, the agent is modeling the world as a mark-off decision process. So it is thinking of taking an action, getting observation and reward, and it's setting the state, the world state that it's the environment state it's using to be the observation. If the world, if it is treating the world as partially observable, then it says the agent state is not the same and it sort of uses things like the history or beliefs about the world state to aggregate the sequence of previous actions taken and the observations are saved and uses that to make its decisions. For example, in something like poker, you get to see your own cards. Other people have cards that are fully affecting the course of the game. But you don't necessarily know what those are. You can see which cards are discarded. And so that's somewhere where it's naturally partially observable. And so you can maintain a belief state over what the other cards are at the other players. And you can use that information or make your decisions. And similarly often in health care, there's a whole bunch of really complicated physiological processes that are going on, but you can monitor parts of them for things like blood pressure, temperature, et cetera, and then use that in order to make decisions. So in terms of types of sequential decision-making processes, one of them is bandits. We'll talk more about this later at the term. Bandits is sort of a really simple version of a Markov decision process. In this sense that the ideas that the actions that are taken have no influence over the next observation. So when might this be reasonable? So let's imagine that you have a series of customers coming to your website. And you show each of them an ad. And then they either click on it or not. And then you get another customer logging in to your website. So in this case, the ad that you show to customer one generally doesn't affect who because which customer two comes along. Now it could maybe really complicated ways, maybe customer one goes to Facebook and says, I really, really love this ad, you should go watch it. But most of the public. and you have to think about this closed loop system of the actions that you're taking changing the state of the world. So the product that I recommend to my customer might affect what the customers opinion is on the next time step. In fact, you hope it will. And so in these cases, we think about the actions actually affecting the state of the world. So another important question is how the world changes. One idea is that it changes deterministically. So when you take an action in a particular state, you go to a different state, but the state you go to is deterministic. There's only one. This is often a pretty common assumption in a lot of robotics and controls. I remember Tomasluzana Perez,, who's a professor over MIT, once it's adjusting to me that if you flip a coin, it's actually a deterministic process. We just model those to the cast eggs. We don't have good enough models. So there are many processes that if you could sort of write down a sufficient perfect model of the world, it would actually look deterministic. But in many cases, even maybe hard to write down those models, and so we're going to approximate them as docastic. And the idea is that then when we take an action, there are many at Pulseball outcomes. So you could just show an add to someone, and they may or may not click on it, and we may just want to represent that with a docastic model. So let's think about a particular example. So if we think about something like Mars rover, when we deploy rovers or robots on really far off planets, it's hard to do communication back and forth. So it'd be nice to be able to make these sort of robots more autonomous. Let's imagine that we have a very simple Mars rover that's thinking about a seven state system. So it's just landed. It's got a particular location and it can either try to go left or try to go the right. I write down, try left or try right, meaning that that's what it's going to try to do, but maybe it's a six-seater fail. Let's imagine that there's different sorts of scientific information to be discovered. And so over an S1, there's a little bit of useful scientific information, but actually over in S7, there's an incredibly rich place where there might be water. And then there's zero in all other states. So we'll go through that as a little bit of an example, as I start to talk about different common components of an RL agent. So one often common component is a model. So a model is simply going to be a representation the agent has for what happens in the world as it takes its actions and what rewards it might get. So in the case of a Markov disengine process, it simply a model that says if I start in this state and I take this action A, what is the distribution over next states I might reach? And it also is going to have a reward model that predicts the expected reward of taking an action in a certain state. So in this case, let's imagine that the reward of the agent is that it thinks that there's zero reward everywhere. And let's imagine that it thinks its motor control is very bad. And so it estimates that whenever it tries to move with 50% probability at stays in the same place and 50% probability it actually moves. Now the model can be wrong. So if you remember what I put up here, the actual reward is that in state S1, you get plus 1 and in state S7, you get S7, you get 10 and everything else you get zero and the reward I just wrote down here is that it's zero everywhere. So this is a totally reasonable reward model the agent could have. It just happens to be wrong. And in many cases the model will be wrong but often can still be used by the agent in useful ways. So the next important component that is always needed by an RL agent is a policy. And the policy or decision policy is simply how we make decisions. Now, because we're thinking about barcops, decision processes here, we're going to think about them as being mappings from states to actions. And a deterministic policy simply means there's one action per state. And a stochastic means you can have a distribution of our actions you might take. So, maybe every time you drive to the airport, you flip a coin, you decide whether you're going to take the back roads or whether you're going to take the highway. So, as a quick check, imagine that in every single state, we do the action try right. Is this the deterministic policy or stochastic policy? Determistic, right? We'll talk more about why deterministic policies are useful and when stochastic policies are useful shortly. Now the value function is the expected discounted sum of future awards under a particular policy. So it's a waiting. It's saying, how much reward do I think I'm going to get both now and in the future? Waded by how much I care about immediate versus long-term rewards. The discount factor gamma is going to be between zero and one. And so the value function then allows us to say sort of how good or bad different states are. So again, in the case of the Mars rover, let's imagine that our discount factor is 0. Our policy is to try to go right. And in this case, say this is our value function. So as with the value of being in state 1 is plus 1, everything else is 0. And the value of being in S7 is 10. Again, this minor might not be the correct value function. It depends also on the true dynamic model. But this is a value function that the agent could have for this policy. Simply tells us what is the expected discounted sum of rewards you'd get. If you follow this policy, start in the state, we weigh each reward by gamut the number of time steps that which you reach it. So when we think about, yeah. So if we wanted to extend this kind of factor to this example, would there be an increasing value or decreasing value to the reward if we have far, far, far. Yes. Question was, if the gamma was not zero here. So gamma is being zero here indicates that essentially we just care about immediate rewards, whether or not we start to sort of eventually start to see like rewards sluined to other states. And the answer is yes. So we'll see more of that next time, but if the discount factor is non-zero, then it basically says you care about not just the immediate reward you get. You're not just myopic, be care about their reward you're gonna get in the future. So in terms of common types of reinforcement learning agents, some of them are model based, which means they maintain in their representation a direct model of how the world works, like a transition model and a reward model, and they may or may not have a policy or value function. They always have to compute a policy, they have to figure out what to do. But they may or may not have an explicit representation for what they would do in any state. Model free approaches have an explicit value function and a policy function and no model. Yeah. Going back with the further slide, I'm confused with when the value function is evaluated. Nice, the one with the seven. Yeah. So why is it not a six that has a value 10 because if you try right at S6 you get to S7? So. You would say well how do I win away think of the rewards happening? We'll talk more about that next time. One really there's many different ways people think of where the rewards happening. Some people think of it as the reward happening for the current state you're in. Some people think of it as the reward happening for the current state you're in. Some people think of it as the reward you're in and the action you take. And some people, some, another common definition is are S.A.S. Prime. Meaning that you don't see what reward you get until you transition. In this particular definition that I'm using here, we're assuming the rewards happened as one year in that state. All of them are basically isomorphic, but we'll try to be careful about which one we're using. The most common one we'll use in the class is SA, which says that when you're in a state and you choose a particular action, then you get a reward and then you transition to your next state. Great question. Okay. So when we think about reinforcement learning agents and whether or not they're maintaining these models and these values and these policies, we get a lot of intersection. So I really like this figure from David Silver, where he thinks about sort of RL algorithms or agents mostly falling into these three different classes. They even have a model or explicit policy or explicit value function. And then there's a whole bunch of algorithms that are serve in the intersection of these. So things like actor critic often have an explicit. And what do I mean by explicit? I mean like often they have a way so that if you give it a stage, you could tell, I could tell you what the value is. But I give you a state, you could tell me immediately what the policy is without additional computation. So, actor-critic combines value functions and policies. There's a lot of algorithms that are also in the intersection of all of these different ones and often in practice it's just very helpful to maintain. Many of these and they have different strengths and weaknesses. For those of you that are interested in the theoretical aspects at Learning Theory, there's some really cool recent work that explicitly looks at what is the formal foundational differences between model based and model free RL that just came out of MSR, Microsoft Research in New York, which indicates that there may be a fundamental gap between model based and model free methods, which on the deep learning slide has been very unclear. So feel free to come ask me about that. So what are the challenges in learning to make good decisions in this sort of framework? One is this issue of planning that we talked about a little bit before, which is even once I've got a model of how the world works, I have to use it to figure out what decisions I should make in a way that I think it's going to allow me to achieve high reward. And in this case, if you're given a model, you can do this planning without any interaction in the real world. So if someone says, here's your transition model and here's your reward model, you can go off and do a bunch of computations on your computer or by paper and decide what the optimal action is to do and then go back to the real world and take that action. It doesn't require any additional experience to compute that. But in reinforcement learning, we have this other additional issue that we might want to think about not just what I think is the best thing for me to do, given the information I have so far. But what is the way I should act so that I can get the information I need in order to make good decisions on the future? So it's like, you know, you go to a brand new restaurant and let's say, let's say move to a new town. You go to there's only one restaurant. You go there the first day and they have five different dishes. You're going to be there for a long time and you want to optimize and get the best dish. And so maybe the first day you try dish one and the second day you try dish two and then the third day three and then et cetera so that you can try everything and then use that to figure out which one is best so that over the long term you pick something that is really delicious. So in this case, the agent has to think explicitly about what decisions it should fake so it can get the information it needs so that in the future it can make good decisions. So in the case of planning and the fact that this is already a hard problem, if you think about something like solitaire, you could already know the rules of the game. It's also two things like go or chats or many other scenarios. And you could know if you take an action, what would be the probability distribution of the next state, and you could use this to compute a potential score. And so using things like tree search or dynamic programming, and we'll talk a lot more about these, particularly the dynamic programming aspect you can use as to decide, given a model of the world, what is the right decision to make? But we're in force about learning itself is a little bit more like solitaire without a rulebook. We're just playing things and you're observing what is happening and you you're trying to get a larger word. And you might use your experience to explicitly compute a model and then plan in that model, or you might not. And you might directly compute a policy or a value function. Now I just want to re-emphasize here this issue of exploration and exploitation. So in the case of the Mars rover, it's only going to learn about how the world works for the actions it tries. So in state S2, if it tries to go left, it can see what happens there, and then from there, it can decide the right next action. Now this is obvious, but it can lead to a dilemma, because it has to be able to balance between things that seem like they might be good based on your prior experience and things that might be good in the future, but perhaps you got unlucky before. So in exploration, we're often interested in trying things that we've never tried before or trying things that so far might have looked bad, but we think in the future might be good, but in exploitation, we're trying things that are expected to be good given the past experience. So here's three examples of this. In the case of movies, exploitation is like watching your favorite movie. The next exploration is watching a new movie that might be good or might be awful. Advertising is showing the ad that's sealed at the most highest click through rate so far. Exploration is showing a different ad. And driving exploitation is trying the fastest road giving's an option case that optimizing for a quiet horizon. You know, it's only good 5A, so you can only live that to your own. It's an option for that horizon because that option times you the same kind of policy as an effect horizon problem or you can repeat the experience of our algorithm. Great question, which is, let's imagine for that example that I gave, that you're only going to be in town for five days. And with the policy that you would compute in that case, if you're in a finite horizon setting, be the same or different as one where you're going to live in this for all of infinite time. We'll talk a little bit more about this next next time, but they're different. And in particular, the normally the policy, if you'll have a finite horizon, is non-stationary, which means that the decision you will make depends on the time step as well as the state. In the infinite horizon case, the assumption is that the optimal policy in the markoff setting is stationary, which means that if you're in the same state, whether you're there on time step three or time step three thousand, you will always do the same thing. But in the finite horizon case, that's not true. And as a critical example of that, so why do we explore, in order to learn information that we can use in the future? So if you're in a finite horizon setting, it's the last day, it's the last day in Hollywood, and you're trying to say what to do. You're not going to explore because there's no benefit from exploration for future, because you're not any more decisions. So in the finitarizing case, the decisions you make have to depend on the value of the information you gave to change your decisions in the remaining horizon. And this often comes up in real cases. Yeah. How much more complicated is if there's a finite horizon, but you don't know where this. I mean, it's I think I remember from game theory this, that's to be very complicated. How does it reflect on this? Question is what about what I would call indefinite horizon problems, where there is a finite horizon, but you know what it is that can get very tricky one way to model it is as an impenet horizon problem with termination states so there are some states which are essentially stings states what you get there the process ends this often happens in games you don't know when the game will end but it's going to be finite and that and so that's one way to put it into the formalism but it is tricky in cases, we tend to model it as impenetrizen and look at the probability of reaching different termination states. The UN miss exploitation and exploration potentially sub-problems. I guess particularly for driving it seems like it'd be better to kind of exploit paths you know we're really good and maybe explore on sub paths you don't know or is good rather than trying like completely granular group. of an incident they might be good. It's a great question. There's generally it is better to enter mix exploration exploitation. In some cases, it is optimal to do all your exploration early or at least equivalent. And then you game from all of that information for later, but it depends on the decision process and we'll spend a significant chunk of the course after the midterm thinking about exploration exploitation. It's definitely a really critical part of reinforcement learning, particularly in high stakes domains. What do I mean by high stakes domains? I mean domains with effect people. So whether it's customers or patients or students, that's where the decisions we make actually affect real people. And so we want to try to learn as quickly as possible. It made good decisions as quickly as we can. Any other questions about this? can you do anything better than random or can you somehow use your prior experience? One of the really great things about doing generalization means that we're going to use state features either learned by deep learning or some other representation to try to share information so that even though the needs the state might not be one you've ever exactly visited before you can share prior information to try to inform what might be a good action to do. Of course, if you share in the wrong direction you can make the wrong decision decision. So if you over-generalize, you can over fit to your if they're like, hey, this is what you should do. This is what your agent should do. This is how your robot should act in the world to evaluate how good it is. So we want to be able to often figure out, you know, your manager says, oh, I think this is the right way we should show ads to customers. Can you tell me how good it is? What's the quick through rate? So one really important question is evaluation. And you might not have a model of the world. So you might still have to go out and gather data to try to evaluate this policy, but you just want to know how good it is. You're not trying to make a new policy, at least not yet. You're just trying to see how good this current one is. And then the control problem is optimization. It's saying let's try to find a really good policy. This typically involves as a sub component evaluation, because often we're going to need to know what does best mean. Best means a really good policy. How do we know how good the policy is? We need to do evaluation. Now, one of the really cool aspects of reinforcement learning is that often we can do this evaluation of policy, which means we can use data gathered from other policies to evaluate the counterfactual of what different policies might do. This is really helpful because it means we don't have to try out all policies exhaustively. So, in terms of what these questions look like, if we go back to our Mars rover example, for policy evaluation it would be if someone says your policy is this, in all of your states, the action you should take is try right. This is the discount factor I care about. Please compute for me or evaluate for me what is the value of this policy. In the control case, they would say, I don't know what the policy should be. I just want you to give me whatever policy has the highest expected discounted summer rewards. And there's actually a key question here, is, okay, expected discounted summer rewards from what? So they might care about a particular starting state. They might say, I want you to figure out the best policy assuming I'm starting from S4. They might say I want you to compute the best policy from all starting states or sort of some average. So in terms of the rest of the course, what we're going to find out what policy or how I use what I've learned so far. In addition to such an early occasion up there, but the board of the rewards, the CKGENC. Great question, which is, okay, let's say I have a policy to start with. I'm evaluating it. And I don't know what the reward function is and I don't know what the optimal policy is and it turns out this current one isn't very good. Do I need to sort of restart or can I use that prior experience to just to live and form what's the next next policy I try or perhaps a whole suite of different policies. In general, you can use the prior experience in order to inform what the next policy is that you try or next suite of policies. There's a little bit of a caveat there, which is you need to have some stochacity in the actions you take. So if you only take the same one action in a state, you can't really learn about any other actions you would take. So you need to assume some sort of generalization or some sort of stochasticity in your policy in order for that information to be useful to try to evaluate other policies. This is a really important issue. This is the issue of counterfactual reasoning and how do we use our old data to figure out how we should act in the future if the old policies may not be the optimal ones. So in general we can and we'll talk a lot about that. It's a really important issue. So we're first going to start off talking about some Mark Up decision process is planning and talking about how do we sort of do this evaluation and both whom we know how the world works meaning that we're given a transition model and a reward model and when we're not, then we're gonna also go to talk about model free policy evaluation and then model free control. We're gonna then spend some time on deep, deep reinforcement learning and reinforcement learning general with function approximation, which is a hugely growing area right now. I thought about making a plot of how many papers are going on in this area right now. It's pretty incredible. And then we're going to talk about policy search, which I think in practice, particularly in robotics is one of the most influential methods right now. And then we're going to spend quite a lot of time on exploration, as well as have a few advanced topics. So just to summarize what we've done today, it's talk a little bit about reinforcement learning, how it differs compared to other aspects of AI machine learning. We went through course logistics and started to talk about sequential decision making under uncertainty. Just as a quick note for next time, we will try to post the lecture slides two days in advance or by the end of two, you know, the evening of two days in advance so that you can print them out if you want to in class and I'll see you guys in what's name.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i}: {doc.metadata['source']}\")"
   ],
   "id": "d885b88e9fe79286",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docs_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])",
   "id": "ae621737130f30c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docs_sorted[0]",
   "id": "17fcc167da943139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, doc in enumerate(docs_sorted):\n",
    "    print(f\"Document {i}: {doc.metadata['source']}\")"
   ],
   "id": "9e3e1c33f063f0e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from langchain_text_splitters import RecursiveCharacterTextSplitter",
   "id": "70bb4b9b159438b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_documents(documents: List[Document]):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Number of chunks: {len(chunks)}, for {len(documents)} documents\")\n",
    "    print(f\"Split documents done !\")\n",
    "    print(type(chunks))\n",
    "    return chunks"
   ],
   "id": "ce78a21e3ac35baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks = split_documents(docs)",
   "id": "ede76895b043d4f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks[99]",
   "id": "393f9c2ca1a2a7cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab05aee28105994b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f9137c188515db83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "15b047f4fdfac1fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "53d9c4f1b135db43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ],
   "id": "eed06ecc82deac4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine doc\n",
    "combined_docs = [doc.page_content for doc in docs]\n",
    "text = \" \".join(combined_docs)"
   ],
   "id": "60a4dddec9a69bb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split them\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_text(text)"
   ],
   "id": "23a2f939ff42e78d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build an index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_texts(splits, embeddings)"
   ],
   "id": "7dcd190d278b87b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build a QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    ")"
   ],
   "id": "1612ab6327de09ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ask a question!\n",
    "query = \"What are the key points to make the AI good decision ?\"\n",
    "qa_chain.invoke(query)"
   ],
   "id": "96099097aec4099f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
